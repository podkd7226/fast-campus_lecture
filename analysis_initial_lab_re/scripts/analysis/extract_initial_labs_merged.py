#!/usr/bin/env python3
"""
ì„ íƒì  ItemID í†µí•© ìŠ¤í¬ë¦½íŠ¸
- ì•ˆì „í•œ ê²½ìš°ë§Œ itemid í†µí•© (í•œìª½ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°)
- ë‘˜ ë‹¤ í™œì„±ì¸ ê²½ìš°ëŠ” í†µí•©í•˜ì§€ ì•ŠìŒ
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ê²½ë¡œ ì„¤ì •
BASE_PATH = '/Users/hyungjun/Desktop/fast campus_lecture'
OUTPUT_PATH = os.path.join(BASE_PATH, 'analysis_initial_lab_re')
DATA_PATH = os.path.join(OUTPUT_PATH, 'data')

def load_merge_rules():
    """í†µí•© ê·œì¹™ ë¡œë“œ ë° ìƒì„±"""
    print("=" * 70)
    print("1. í†µí•© ê·œì¹™ ìƒì„±")
    print("=" * 70)
    
    # ê°œì„  ê°€ëŠ¥ í•­ëª© (í•œìª½ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°)
    improvable = pd.read_csv(os.path.join(DATA_PATH, 'improvable_items.csv'))
    
    # ì¤‘ë³µ í™œì„± ë¼ë²¨ (ë‘˜ ë‹¤ ë°ì´í„° ìˆëŠ” ê²½ìš°)
    duplicate_active = pd.read_csv(os.path.join(DATA_PATH, 'duplicate_active_labels.csv'))
    
    # í†µí•© ë¶ˆê°€ ë¼ë²¨ (ë‘˜ ë‹¤ í™œì„±)
    no_merge_labels = set(duplicate_active['label'].tolist())
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"   - ê°œì„  ê°€ëŠ¥ í•­ëª©: {len(improvable)}ê°œ")
    print(f"   - ì¤‘ë³µ í™œì„± ë¼ë²¨: {len(duplicate_active)}ê°œ (í†µí•© ë¶ˆê°€)")
    
    # ì•ˆì „í•œ í†µí•© ê·œì¹™ ìƒì„±
    safe_merge_rules = {}
    unsafe_merges = []
    
    for _, row in improvable.iterrows():
        label = row['label']
        
        # ì¤‘ë³µ í™œì„± ë¼ë²¨ì€ ì œì™¸
        if label in no_merge_labels:
            unsafe_merges.append({
                'label': label,
                'reason': 'ë‘˜ ë‹¤ í™œì„± (ê°’ ì°¨ì´ ì¡´ì¬)'
            })
            continue
        
        # ë¹ˆ itemidë“¤
        empty_itemids = str(row['empty_itemids']).split(';')
        # í™œì„± itemidë“¤
        active_itemids = str(row['active_itemids']).split(';')
        
        # ì²« ë²ˆì§¸ í™œì„± itemidë¡œ í†µí•©
        target_itemid = int(active_itemids[0])
        
        for empty_id in empty_itemids:
            try:
                empty_id = int(empty_id)
                safe_merge_rules[empty_id] = target_itemid
            except:
                continue
    
    print(f"\nâœ… í†µí•© ê·œì¹™ ìƒì„± ì™„ë£Œ:")
    print(f"   - ì•ˆì „í•œ í†µí•©: {len(safe_merge_rules)}ê°œ itemid")
    print(f"   - í†µí•© ë¶ˆê°€: {len(unsafe_merges)}ê°œ ë¼ë²¨")
    
    if unsafe_merges:
        print(f"\nâš ï¸ í†µí•© ë¶ˆê°€ í•­ëª©:")
        for item in unsafe_merges:
            print(f"   - {item['label']}: {item['reason']}")
    
    return safe_merge_rules, unsafe_merges

def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    print("\n" + "=" * 70)
    print("2. ë°ì´í„° ë¡œë”©")
    print("=" * 70)
    
    # ì…ì› ë°ì´í„°
    admissions = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/core/admissions_sampled.csv'))
    admissions['admittime'] = pd.to_datetime(admissions['admittime'])
    admissions['admit_date'] = admissions['admittime'].dt.date
    
    # ê²€ì‚¬ ë°ì´í„°
    labevents = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/hosp/labevents_sampled.csv'))
    labevents['charttime'] = pd.to_datetime(labevents['charttime'])
    labevents['chart_date'] = labevents['charttime'].dt.date
    
    # inclusion ì •ë³´
    inclusion_df = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/hosp/d_labitems_inclusion.csv'))
    included_labs = inclusion_df[inclusion_df['inclusion'] == 1].copy()
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   - ì…ì›: {len(admissions):,}ê±´")
    print(f"   - ì „ì²´ ê²€ì‚¬: {len(labevents):,}ê±´")
    print(f"   - Inclusion=1 í•­ëª©: {len(included_labs)}ê°œ")
    
    return admissions, labevents, included_labs

def apply_merge_rules(labevents, merge_rules):
    """í†µí•© ê·œì¹™ ì ìš©"""
    print("\n" + "=" * 70)
    print("3. ItemID í†µí•© ì ìš©")
    print("=" * 70)
    
    # ì›ë³¸ ë°±ì—…
    original_counts = labevents['itemid'].value_counts()
    
    # í†µí•© ì ìš©
    merged_count = 0
    for old_id, new_id in merge_rules.items():
        mask = labevents['itemid'] == old_id
        count = mask.sum()
        if count > 0:
            labevents.loc[mask, 'itemid'] = new_id
            merged_count += count
            print(f"   - {old_id} â†’ {new_id}: {count}ê±´ í†µí•©")
    
    # í†µí•© í›„ í†µê³„
    new_counts = labevents['itemid'].value_counts()
    
    print(f"\nâœ… í†µí•© ì™„ë£Œ:")
    print(f"   - í†µí•©ëœ ë ˆì½”ë“œ: {merged_count:,}ê±´")
    print(f"   - ê³ ìœ  itemid: {len(original_counts)} â†’ {len(new_counts)}")
    
    return labevents

def create_merged_lab_items(included_labs, merge_rules):
    """í†µí•©ëœ ê²€ì‚¬ í•­ëª© ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
    print("\n" + "=" * 70)
    print("4. í†µí•©ëœ ê²€ì‚¬ í•­ëª© ìƒì„±")
    print("=" * 70)
    
    # í†µí•© ëŒ€ìƒ itemidë“¤
    merged_itemids = set(merge_rules.keys())
    
    LAB_ITEMS = {}
    LAB_METADATA = {}
    
    for _, row in included_labs.iterrows():
        itemid = row['itemid']
        
        # í†µí•©ë˜ëŠ” itemidëŠ” ê±´ë„ˆë›°ê¸°
        if itemid in merged_itemids:
            continue
        
        original_label = row['label']
        
        # ë¼ë²¨ ì •ë¦¬
        clean_label = (original_label
                      .replace(' ', '_')
                      .replace(',', '_')
                      .replace('(', '')
                      .replace(')', '')
                      .replace('/', '_')
                      .replace('-', '_'))
        
        # í†µí•© ëŒ€ìƒì¸ ê²½ìš° í‘œì‹œ
        if itemid in merge_rules.values():
            # í†µí•©ëœ itemid ì°¾ê¸°
            merged_from = [k for k, v in merge_rules.items() if v == itemid]
            unique_label = f"{clean_label}_{itemid}_merged"
        else:
            unique_label = f"{clean_label}_{itemid}"
        
        LAB_ITEMS[itemid] = unique_label
        LAB_METADATA[itemid] = {
            'original_label': original_label,
            'clean_label': clean_label,
            'unique_label': unique_label,
            'category': row.get('category', ''),
            'fluid': row.get('fluid', ''),
            'merged_from': merged_from if itemid in merge_rules.values() else []
        }
    
    print(f"âœ… í†µí•©ëœ ê²€ì‚¬ í•­ëª©: {len(LAB_ITEMS)}ê°œ")
    
    # í†µí•© í†µê³„
    merged_targets = [v for v in merge_rules.values()]
    unique_targets = set(merged_targets)
    print(f"   - í†µí•© ëŒ€ìƒ itemid: {len(unique_targets)}ê°œ")
    print(f"   - ì œê±°ëœ itemid: {len(merged_itemids)}ê°œ")
    
    return LAB_ITEMS, LAB_METADATA

def extract_labs_with_window(admissions, labevents, LAB_ITEMS):
    """ì‹œê°„ ìœˆë„ìš° ì ìš©í•˜ì—¬ ê²€ì‚¬ ì¶”ì¶œ"""
    print("\n" + "=" * 70)
    print("5. ì‹œê°„ ìœˆë„ìš° ê²€ì‚¬ ì¶”ì¶œ")
    print("=" * 70)
    
    # inclusion=1 itemidë§Œ í•„í„°ë§
    lab_itemids = list(LAB_ITEMS.keys())
    labevents_filtered = labevents[labevents['itemid'].isin(lab_itemids)].copy()
    
    print(f"âœ… í•„í„°ë§: {len(labevents_filtered):,}ê±´")
    
    results = []
    offset_info = []
    
    for idx, admission in admissions.iterrows():
        if idx % 200 == 0 and idx > 0:
            print(f"   ì²˜ë¦¬ ì§„í–‰: {idx}/{len(admissions)}")
            
        hadm_id = admission['hadm_id']
        subject_id = admission['subject_id']
        admit_date = pd.to_datetime(admission['admit_date']).date()
        
        # 3ì¼ ìœˆë„ìš°
        date_minus1 = admit_date - timedelta(days=1)
        date_plus1 = admit_date + timedelta(days=1)
        
        # ê° ë‚ ì§œë³„ ê²€ì‚¬
        labs_by_day = {
            0: labevents_filtered[
                ((labevents_filtered['hadm_id'] == hadm_id) | 
                 (labevents_filtered['subject_id'] == subject_id)) & 
                (labevents_filtered['chart_date'] == admit_date)
            ],
            -1: labevents_filtered[
                (labevents_filtered['subject_id'] == subject_id) & 
                (labevents_filtered['chart_date'] == date_minus1)
            ],
            1: labevents_filtered[
                ((labevents_filtered['hadm_id'] == hadm_id) | 
                 (labevents_filtered['subject_id'] == subject_id)) & 
                (labevents_filtered['chart_date'] == date_plus1)
            ]
        }
        
        # ê° itemidë³„ ì²˜ë¦¬
        for itemid, lab_name in LAB_ITEMS.items():
            selected_data = None
            selected_day = None
            
            # ìš°ì„ ìˆœìœ„: Day 0 > Day -1 > Day +1
            for day in [0, -1, 1]:
                day_labs = labs_by_day[day]
                item_labs = day_labs[day_labs['itemid'] == itemid]
                if len(item_labs) > 0:
                    selected_data = item_labs.iloc[0]
                    selected_day = day
                    break
            
            if selected_data is not None:
                results.append({
                    'hadm_id': hadm_id,
                    'subject_id': subject_id,
                    'admit_date': admit_date,
                    'itemid': itemid,
                    'lab_name': lab_name,
                    'charttime': selected_data['charttime'],
                    'chart_date': selected_data['chart_date'],
                    'valuenum': selected_data['valuenum'],
                    'value': selected_data.get('value', ''),
                    'valueuom': selected_data.get('valueuom', ''),
                    'flag': selected_data.get('flag', ''),
                    'ref_range_lower': selected_data.get('ref_range_lower', np.nan),
                    'ref_range_upper': selected_data.get('ref_range_upper', np.nan)
                })
                
                offset_info.append({
                    'hadm_id': hadm_id,
                    'itemid': itemid,
                    'lab_name': lab_name,
                    'day_offset': selected_day,
                    'source': f"Day{selected_day:+d}" if selected_day != 0 else "Day0"
                })
    
    long_df = pd.DataFrame(results) if results else pd.DataFrame()
    offset_df = pd.DataFrame(offset_info) if offset_info else pd.DataFrame()
    
    print(f"\nâœ… ì¶”ì¶œ ì™„ë£Œ:")
    print(f"   - ê²€ì‚¬ ë ˆì½”ë“œ: {len(long_df):,}ê±´")
    print(f"   - ê³ ìœ  itemid: {long_df['itemid'].nunique() if len(long_df) > 0 else 0}ê°œ")
    
    return long_df, offset_df

def create_wide_format(admissions, long_df, LAB_ITEMS):
    """Wide format ìƒì„±"""
    print("\n" + "=" * 70)
    print("6. Wide Format ë³€í™˜")
    print("=" * 70)
    
    # ê¸°ë³¸ ì…ì› ì •ë³´
    wide_df = admissions[['hadm_id', 'subject_id', 'admittime', 
                          'hospital_expire_flag', 'deathtime']].copy()
    wide_df['admit_date'] = pd.to_datetime(wide_df['admittime']).dt.date
    
    # ëª¨ë“  ê²€ì‚¬ ì»¬ëŸ¼ ì´ˆê¸°í™”
    for itemid, lab_name in LAB_ITEMS.items():
        wide_df[lab_name] = np.nan
    
    # ì‹¤ì œ ë°ì´í„° ì±„ìš°ê¸°
    if not long_df.empty:
        pivot_df = long_df.pivot_table(
            index='hadm_id',
            columns='lab_name',
            values='valuenum',
            aggfunc='first'
        )
        
        for hadm_id in pivot_df.index:
            if hadm_id in wide_df['hadm_id'].values:
                idx = wide_df[wide_df['hadm_id'] == hadm_id].index[0]
                for col in pivot_df.columns:
                    if col in wide_df.columns:
                        wide_df.loc[idx, col] = pivot_df.loc[hadm_id, col]
    
    # í†µê³„
    lab_columns = [col for col in wide_df.columns 
                   if col not in ['hadm_id', 'subject_id', 'admittime', 
                                 'hospital_expire_flag', 'deathtime', 'admit_date']]
    
    print(f"âœ… Wide format ìƒì„± ì™„ë£Œ")
    print(f"   - ì°¨ì›: {wide_df.shape[0]} Ã— {len(lab_columns)} ê²€ì‚¬")
    
    # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
    non_null_counts = {}
    for col in lab_columns:
        non_null = wide_df[col].notna().sum()
        if non_null > 0:
            non_null_counts[col] = non_null
    
    has_any_lab = ~wide_df[lab_columns].isna().all(axis=1)
    
    print(f"\nğŸ“Š ì»¤ë²„ë¦¬ì§€:")
    print(f"   - ë°ì´í„° ìˆëŠ” ì»¬ëŸ¼: {len(non_null_counts)}/{len(lab_columns)}")
    print(f"   - ê²€ì‚¬ ìˆëŠ” ì…ì›: {has_any_lab.sum()}/{len(wide_df)} "
          f"({has_any_lab.sum()/len(wide_df)*100:.1f}%)")
    
    return wide_df, lab_columns

def save_results(wide_df, long_df, offset_df, merge_rules, unsafe_merges, 
                 LAB_ITEMS, LAB_METADATA, lab_columns):
    """ê²°ê³¼ ì €ì¥"""
    print("\n" + "=" * 70)
    print("7. ê²°ê³¼ ì €ì¥")
    print("=" * 70)
    
    # CSV íŒŒì¼ ì €ì¥
    wide_df.to_csv(os.path.join(DATA_PATH, 'labs_initial_merged_wide.csv'), index=False)
    long_df.to_csv(os.path.join(DATA_PATH, 'labs_initial_merged_long.csv'), index=False)
    offset_df.to_csv(os.path.join(DATA_PATH, 'labs_merged_offset_info.csv'), index=False)
    
    # í†µí•© ë§¤í•‘ í…Œì´ë¸” ì €ì¥
    merge_mapping = []
    for old_id, new_id in merge_rules.items():
        if new_id in LAB_METADATA:
            merge_mapping.append({
                'old_itemid': old_id,
                'new_itemid': new_id,
                'label': LAB_METADATA[new_id]['original_label']
            })
    
    if merge_mapping:
        merge_mapping_df = pd.DataFrame(merge_mapping)
        merge_mapping_df.to_csv(os.path.join(DATA_PATH, 'merge_mapping.csv'), index=False)
    
    # ë©”íƒ€ë°ì´í„°
    metadata = {
        'extraction_info': {
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'method': 'selective_itemid_merging',
            'description': 'ì•ˆì „í•œ ê²½ìš°ë§Œ itemid í†µí•© (í•œìª½ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°)'
        },
        'merge_statistics': {
            'total_merge_rules': len(merge_rules),
            'merged_itemids': list(merge_rules.keys()),
            'target_itemids': list(set(merge_rules.values())),
            'unsafe_merges': len(unsafe_merges),
            'unsafe_labels': [item['label'] for item in unsafe_merges]
        },
        'data_summary': {
            'original_itemids': 87,
            'final_itemids': len(LAB_ITEMS),
            'total_admissions': len(wide_df),
            'total_lab_records': len(long_df),
            'columns_with_data': len([c for c in lab_columns 
                                     if wide_df[c].notna().sum() > 0])
        },
        'coverage': {
            'admissions_with_labs': int((~wide_df[lab_columns].isna().all(axis=1)).sum()),
            'coverage_rate': float((~wide_df[lab_columns].isna().all(axis=1)).sum() / 
                                 len(wide_df) * 100)
        }
    }
    
    with open(os.path.join(DATA_PATH, 'merge_summary.json'), 'w') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"   - labs_initial_merged_wide.csv: {wide_df.shape}")
    print(f"   - labs_initial_merged_long.csv: {len(long_df):,} records")
    print(f"   - merge_mapping.csv: {len(merge_mapping)} mappings")
    print(f"   - merge_summary.json")
    
    return metadata

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "ğŸ”„ " * 20)
    print(" ì„ íƒì  ItemID í†µí•© ì²˜ë¦¬")
    print("ğŸ”„ " * 20)
    
    # 1. í†µí•© ê·œì¹™ ìƒì„±
    merge_rules, unsafe_merges = load_merge_rules()
    
    # 2. ë°ì´í„° ë¡œë“œ
    admissions, labevents, included_labs = load_data()
    
    # 3. í†µí•© ê·œì¹™ ì ìš©
    labevents = apply_merge_rules(labevents, merge_rules)
    
    # 4. í†µí•©ëœ ê²€ì‚¬ í•­ëª© ìƒì„±
    LAB_ITEMS, LAB_METADATA = create_merged_lab_items(included_labs, merge_rules)
    
    # 5. ì‹œê°„ ìœˆë„ìš° ê²€ì‚¬ ì¶”ì¶œ
    long_df, offset_df = extract_labs_with_window(admissions, labevents, LAB_ITEMS)
    
    # 6. Wide format ë³€í™˜
    wide_df, lab_columns = create_wide_format(admissions, long_df, LAB_ITEMS)
    
    # 7. ê²°ê³¼ ì €ì¥
    metadata = save_results(wide_df, long_df, offset_df, merge_rules, unsafe_merges,
                          LAB_ITEMS, LAB_METADATA, lab_columns)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ì„ íƒì  ItemID í†µí•© ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   - ItemID: 87ê°œ â†’ {metadata['data_summary']['final_itemids']}ê°œ")
    print(f"   - ì•ˆì „í•˜ê²Œ í†µí•©: {len(merge_rules)}ê°œ")
    print(f"   - í†µí•© ë¶ˆê°€: {len(unsafe_merges)}ê°œ (ê°’ ì°¨ì´ ì¡´ì¬)")
    print(f"   - ì»¤ë²„ë¦¬ì§€: {metadata['coverage']['coverage_rate']:.1f}%")

if __name__ == "__main__":
    main()