#!/usr/bin/env python3
"""
MIMIC-IV ì´ˆê¸° í˜ˆì•¡ê²€ì‚¬ ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ (ì •ë¦¬ëœ ë²„ì „)
- 87ê°œ inclusion=1 itemid ëª¨ë‘ ê°œë³„ ì²˜ë¦¬
- offset ì •ë³´ëŠ” ë³„ë„ íŒŒì¼ë¡œ ë¶„ë¦¬ ì €ì¥
- itemid ê¸°ë°˜ ì²˜ë¦¬ (ì¤‘ë³µ ë¼ë²¨ ë°©ì§€)
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ê²½ë¡œ ì„¤ì •
BASE_PATH = '/Users/hyungjun/Desktop/fast campus_lecture'
OUTPUT_PATH = os.path.join(BASE_PATH, 'analysis_initial_lab_re')
FIGURE_PATH = os.path.join(OUTPUT_PATH, 'figures')
DATA_PATH = os.path.join(OUTPUT_PATH, 'data')

# ì¶œë ¥ í´ë” ìƒì„±
os.makedirs(FIGURE_PATH, exist_ok=True)
os.makedirs(DATA_PATH, exist_ok=True)

def load_inclusion_items():
    """inclusion=1ì¸ ëª¨ë“  ê²€ì‚¬ í•­ëª© ë¡œë“œ ë° ì •ë¦¬"""
    print("=" * 70)
    print("1. INCLUSION=1 ê²€ì‚¬ í•­ëª© ë¡œë”©")
    print("=" * 70)
    
    # d_labitems_inclusion.csv ë¡œë“œ
    inclusion_df = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/hosp/d_labitems_inclusion.csv'))
    included_labs = inclusion_df[inclusion_df['inclusion'] == 1].copy()
    
    # itemidë³„ ê³ ìœ  ë¼ë²¨ ìƒì„±
    LAB_ITEMS = {}
    LAB_METADATA = {}
    
    for _, row in included_labs.iterrows():
        itemid = row['itemid']
        original_label = row['label']
        
        # ë¼ë²¨ ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        clean_label = (original_label
                      .replace(' ', '_')
                      .replace(',', '_')
                      .replace('(', '')
                      .replace(')', '')
                      .replace('/', '_')
                      .replace('-', '_'))
        
        # itemid í¬í•¨í•œ ê³ ìœ  ë¼ë²¨
        unique_label = f"{clean_label}_{itemid}"
        
        LAB_ITEMS[itemid] = unique_label
        LAB_METADATA[itemid] = {
            'original_label': original_label,
            'clean_label': clean_label,
            'unique_label': unique_label,
            'category': row.get('category', ''),
            'fluid': row.get('fluid', ''),
            'loinc_code': row.get('loinc_code', '')
        }
    
    print(f"âœ… {len(LAB_ITEMS)}ê°œ ê²€ì‚¬ í•­ëª© ë¡œë“œ ì™„ë£Œ")
    
    # ì¤‘ë³µ ë¼ë²¨ í†µê³„
    label_counts = {}
    for meta in LAB_METADATA.values():
        label = meta['original_label']
        label_counts[label] = label_counts.get(label, 0) + 1
    
    duplicates = {k: v for k, v in label_counts.items() if v > 1}
    if duplicates:
        print(f"\nğŸ“Š ì¤‘ë³µ ë¼ë²¨ í˜„í™©: {len(duplicates)}ê°œ ë¼ë²¨ì´ ì—¬ëŸ¬ itemid ë³´ìœ ")
        for label, count in sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   - {label}: {count}ê°œ itemid")
    
    return LAB_ITEMS, LAB_METADATA

def load_data():
    """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
    print("\n" + "=" * 70)
    print("2. ë°ì´í„° ë¡œë”©")
    print("=" * 70)
    
    # ì…ì› ë°ì´í„°
    admissions = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/core/admissions_sampled.csv'))
    admissions['admittime'] = pd.to_datetime(admissions['admittime'])
    admissions['admit_date'] = admissions['admittime'].dt.date
    
    # í™˜ì ë°ì´í„°
    patients = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/core/patients_sampled.csv'))
    
    # ê²€ì‚¬ ë°ì´í„°
    labevents = pd.read_csv(os.path.join(BASE_PATH, 'processed_data/hosp/labevents_sampled.csv'))
    labevents['charttime'] = pd.to_datetime(labevents['charttime'])
    labevents['chart_date'] = labevents['charttime'].dt.date
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   - ì…ì›: {len(admissions):,}ê±´")
    print(f"   - í™˜ì: {len(patients):,}ëª…")
    print(f"   - ì „ì²´ ê²€ì‚¬: {len(labevents):,}ê±´")
    
    return admissions, patients, labevents

def extract_labs_with_window(admissions, labevents, LAB_ITEMS):
    """ì‹œê°„ ìœˆë„ìš°ë¥¼ ì ìš©í•œ ê²€ì‚¬ ë°ì´í„° ì¶”ì¶œ"""
    print("\n" + "=" * 70)
    print("3. ì‹œê°„ ìœˆë„ìš° ê²€ì‚¬ ì¶”ì¶œ (Day -1, 0, +1)")
    print("=" * 70)
    
    # inclusion=1 itemidë§Œ í•„í„°ë§
    lab_itemids = list(LAB_ITEMS.keys())
    labevents_filtered = labevents[labevents['itemid'].isin(lab_itemids)].copy()
    
    print(f"âœ… Inclusion=1 í•„í„°ë§: {len(labevents_filtered):,}ê±´")
    
    # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” itemid í™•ì¸
    available_itemids = set(labevents_filtered['itemid'].unique())
    missing_itemids = set(lab_itemids) - available_itemids
    
    print(f"   - ë°ì´í„° ìˆëŠ” itemid: {len(available_itemids)}ê°œ")
    print(f"   - ë°ì´í„° ì—†ëŠ” itemid: {len(missing_itemids)}ê°œ")
    
    results = []
    offset_info = []
    
    total = len(admissions)
    print(f"\nâ³ {total}ê°œ ì…ì› ì²˜ë¦¬ ì¤‘...")
    
    for idx, admission in admissions.iterrows():
        if idx % 200 == 0 and idx > 0:
            print(f"   ì²˜ë¦¬ ì§„í–‰: {idx}/{total} ({idx/total*100:.1f}%)")
            
        hadm_id = admission['hadm_id']
        subject_id = admission['subject_id']
        admit_date = pd.to_datetime(admission['admit_date']).date()
        
        # 3ì¼ ìœˆë„ìš° ë‚ ì§œ
        date_minus1 = admit_date - timedelta(days=1)
        date_plus1 = admit_date + timedelta(days=1)
        
        # ê° ë‚ ì§œë³„ ê²€ì‚¬ ë°ì´í„°
        labs_by_day = {
            0: labevents_filtered[
                ((labevents_filtered['hadm_id'] == hadm_id) | 
                 (labevents_filtered['subject_id'] == subject_id)) & 
                (labevents_filtered['chart_date'] == admit_date)
            ],
            -1: labevents_filtered[
                (labevents_filtered['subject_id'] == subject_id) & 
                (labevents_filtered['chart_date'] == date_minus1)
            ],
            1: labevents_filtered[
                ((labevents_filtered['hadm_id'] == hadm_id) | 
                 (labevents_filtered['subject_id'] == subject_id)) & 
                (labevents_filtered['chart_date'] == date_plus1)
            ]
        }
        
        # ê° itemidë³„ ì²˜ë¦¬
        for itemid, lab_name in LAB_ITEMS.items():
            # ìš°ì„ ìˆœìœ„: Day 0 > Day -1 > Day +1
            selected_data = None
            selected_day = None
            
            for day in [0, -1, 1]:
                day_labs = labs_by_day[day]
                item_labs = day_labs[day_labs['itemid'] == itemid]
                if len(item_labs) > 0:
                    selected_data = item_labs.iloc[0]
                    selected_day = day
                    break
            
            # ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥
            if selected_data is not None:
                results.append({
                    'hadm_id': hadm_id,
                    'subject_id': subject_id,
                    'admit_date': admit_date,
                    'itemid': itemid,
                    'lab_name': lab_name,
                    'charttime': selected_data['charttime'],
                    'chart_date': selected_data['chart_date'],
                    'valuenum': selected_data['valuenum'],
                    'value': selected_data.get('value', ''),
                    'valueuom': selected_data.get('valueuom', ''),
                    'flag': selected_data.get('flag', ''),
                    'ref_range_lower': selected_data.get('ref_range_lower', np.nan),
                    'ref_range_upper': selected_data.get('ref_range_upper', np.nan)
                })
                
                # Offset ì •ë³´ ë³„ë„ ì €ì¥
                offset_info.append({
                    'hadm_id': hadm_id,
                    'itemid': itemid,
                    'lab_name': lab_name,
                    'day_offset': selected_day,
                    'source': f"Day{selected_day:+d}" if selected_day != 0 else "Day0"
                })
    
    print(f"\nâœ… ì¶”ì¶œ ì™„ë£Œ")
    print(f"   - ì¶”ì¶œëœ ê²€ì‚¬: {len(results):,}ê±´")
    print(f"   - Offset ì •ë³´: {len(offset_info):,}ê±´")
    
    # DataFrame ìƒì„±
    long_df = pd.DataFrame(results) if results else pd.DataFrame()
    offset_df = pd.DataFrame(offset_info) if offset_info else pd.DataFrame()
    
    # ë°ì´í„° ì¶œì²˜ í†µê³„
    if not offset_df.empty:
        source_stats = offset_df.groupby('source').size()
        print(f"\nğŸ“Š ë°ì´í„° ì¶œì²˜ ë¶„í¬:")
        for source, count in source_stats.sort_index().items():
            pct = count / len(offset_df) * 100
            print(f"   - {source}: {count:,}ê±´ ({pct:.1f}%)")
    
    return long_df, offset_df

def create_wide_format(admissions, long_df, LAB_ITEMS):
    """Wide format ë³€í™˜ (offset ì œì™¸)"""
    print("\n" + "=" * 70)
    print("4. Wide Format ë³€í™˜")
    print("=" * 70)
    
    # ê¸°ë³¸ ì…ì› ì •ë³´ë¡œ ì‹œì‘
    wide_df = admissions[['hadm_id', 'subject_id', 'admittime', 
                          'hospital_expire_flag', 'deathtime']].copy()
    wide_df['admit_date'] = pd.to_datetime(wide_df['admittime']).dt.date
    
    # ëª¨ë“  87ê°œ ê²€ì‚¬ ì»¬ëŸ¼ì„ NaNìœ¼ë¡œ ì´ˆê¸°í™”
    print("â³ 87ê°œ ê²€ì‚¬ ì»¬ëŸ¼ ì´ˆê¸°í™” ì¤‘...")
    for itemid, lab_name in LAB_ITEMS.items():
        wide_df[lab_name] = np.nan
    
    # ì‹¤ì œ ë°ì´í„°ë¡œ ì±„ìš°ê¸°
    if not long_df.empty:
        print("â³ ì‹¤ì œ ê²€ì‚¬ ë°ì´í„° ì±„ìš°ê¸° ì¤‘...")
        
        # Pivot table ìƒì„±
        pivot_df = long_df.pivot_table(
            index='hadm_id',
            columns='lab_name',
            values='valuenum',
            aggfunc='first'  # ì¤‘ë³µ ì‹œ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
        )
        
        # Wide formatì— ë³‘í•©
        for hadm_id in pivot_df.index:
            if hadm_id in wide_df['hadm_id'].values:
                idx = wide_df[wide_df['hadm_id'] == hadm_id].index[0]
                for col in pivot_df.columns:
                    if col in wide_df.columns:
                        wide_df.loc[idx, col] = pivot_df.loc[hadm_id, col]
    
    # í†µê³„ ê³„ì‚°
    lab_columns = [col for col in wide_df.columns 
                   if col not in ['hadm_id', 'subject_id', 'admittime', 
                                 'hospital_expire_flag', 'deathtime', 'admit_date']]
    
    print(f"\nâœ… Wide format ìƒì„± ì™„ë£Œ")
    print(f"   - ì°¨ì›: {wide_df.shape[0]} ì…ì› Ã— {len(lab_columns)} ê²€ì‚¬")
    print(f"   - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(wide_df.columns)}ê°œ")
    
    # ë°ì´í„° ê°€ìš©ì„± ë¶„ì„
    non_null_counts = {}
    for col in lab_columns:
        non_null = wide_df[col].notna().sum()
        if non_null > 0:
            non_null_counts[col] = non_null
    
    print(f"\nğŸ“Š ë°ì´í„° ê°€ìš©ì„±:")
    print(f"   - ë°ì´í„° ìˆëŠ” ì»¬ëŸ¼: {len(non_null_counts)}ê°œ")
    print(f"   - ì™„ì „ ë¹„ì–´ìˆëŠ” ì»¬ëŸ¼: {len(lab_columns) - len(non_null_counts)}ê°œ")
    
    # Top 5 ê²€ì‚¬
    if non_null_counts:
        top_labs = sorted(non_null_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"\nğŸ“ˆ ë°ì´í„° ìµœë‹¤ ê²€ì‚¬ Top 5:")
        for lab, count in top_labs:
            print(f"   - {lab}: {count}ê±´ ({count/len(wide_df)*100:.1f}%)")
    
    # ì…ì›ë³„ í†µê³„
    has_any_lab = ~wide_df[lab_columns].isna().all(axis=1)
    print(f"\nğŸ“Š ì…ì›ë³„ ê²€ì‚¬ ë³´ìœ ìœ¨:")
    print(f"   - ê²€ì‚¬ ìˆìŒ: {has_any_lab.sum()}ê±´ ({has_any_lab.sum()/len(wide_df)*100:.1f}%)")
    print(f"   - ê²€ì‚¬ ì—†ìŒ: {(~has_any_lab).sum()}ê±´")
    
    return wide_df, lab_columns

def save_results(long_df, wide_df, offset_df, LAB_ITEMS, LAB_METADATA, lab_columns):
    """ê²°ê³¼ ì €ì¥"""
    print("\n" + "=" * 70)
    print("5. ê²°ê³¼ ì €ì¥")
    print("=" * 70)
    
    # CSV íŒŒì¼ ì €ì¥
    print("ğŸ’¾ CSV íŒŒì¼ ì €ì¥ ì¤‘...")
    
    # ë©”ì¸ ë°ì´í„° (offset ì œì™¸)
    wide_df.to_csv(os.path.join(DATA_PATH, 'labs_initial_wide.csv'), index=False)
    long_df.to_csv(os.path.join(DATA_PATH, 'labs_initial_long.csv'), index=False)
    
    # Offset ì •ë³´ ë³„ë„ ì €ì¥
    offset_df.to_csv(os.path.join(DATA_PATH, 'labs_offset_info.csv'), index=False)
    
    # ê²€ì‚¬ í•­ëª© ìš”ì•½
    lab_summary = []
    for itemid, unique_label in LAB_ITEMS.items():
        meta = LAB_METADATA[itemid]
        if unique_label in lab_columns:
            non_null = wide_df[unique_label].notna().sum()
        else:
            non_null = 0
            
        lab_summary.append({
            'itemid': itemid,
            'unique_label': unique_label,
            'original_label': meta['original_label'],
            'category': meta['category'],
            'fluid': meta['fluid'],
            'has_data': non_null > 0,
            'data_count': non_null,
            'coverage_pct': non_null / len(wide_df) * 100
        })
    
    lab_summary_df = pd.DataFrame(lab_summary)
    lab_summary_df = lab_summary_df.sort_values('data_count', ascending=False)
    lab_summary_df.to_csv(os.path.join(DATA_PATH, 'lab_items_summary.csv'), index=False)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    print("ğŸ“ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì»¬ëŸ¼ë³„ í†µê³„
    column_stats = {}
    for col in lab_columns:
        non_null = wide_df[col].notna().sum()
        column_stats[col] = {
            'non_null_count': int(non_null),
            'null_count': int(len(wide_df) - non_null),
            'coverage_rate': float(non_null / len(wide_df) * 100)
        }
    
    # ì „ì²´ ë©”íƒ€ë°ì´í„°
    metadata = {
        'extraction_info': {
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'method': 'itemid_based_individual_columns',
            'time_window': 'Day -1, 0, +1 priority',
            'offset_handling': 'separated_to_different_file'
        },
        'data_summary': {
            'total_admissions': len(wide_df),
            'total_patients': wide_df['subject_id'].nunique(),
            'total_lab_items': len(LAB_ITEMS),
            'columns_with_data': len([c for c in column_stats if column_stats[c]['non_null_count'] > 0]),
            'empty_columns': len([c for c in column_stats if column_stats[c]['non_null_count'] == 0])
        },
        'coverage': {
            'admissions_with_any_lab': int((~wide_df[lab_columns].isna().all(axis=1)).sum()),
            'coverage_rate': float((~wide_df[lab_columns].isna().all(axis=1)).sum() / len(wide_df) * 100),
            'total_lab_records': len(long_df)
        },
        'source_distribution': offset_df.groupby('source').size().to_dict() if not offset_df.empty else {},
        'file_info': {
            'labs_initial_wide.csv': {
                'rows': len(wide_df),
                'columns': len(wide_df.columns),
                'description': 'Wide format with all 87 lab columns (no offset)'
            },
            'labs_initial_long.csv': {
                'rows': len(long_df),
                'description': 'Long format with all lab records'
            },
            'labs_offset_info.csv': {
                'rows': len(offset_df),
                'description': 'Day offset information for each lab'
            }
        }
    }
    
    with open(os.path.join(DATA_PATH, 'labs_metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"   - labs_initial_wide.csv: {wide_df.shape}")
    print(f"   - labs_initial_long.csv: {len(long_df):,} records")
    print(f"   - labs_offset_info.csv: {len(offset_df):,} records")
    print(f"   - lab_items_summary.csv: {len(lab_summary_df)} items")
    print(f"   - labs_metadata.json: Complete metadata")
    
    return metadata

def print_final_summary(metadata):
    """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ìµœì¢… ìš”ì•½")
    print("=" * 70)
    
    print(f"\nâœ… ì¶”ì¶œ ì™„ë£Œ")
    print(f"   - ì²˜ë¦¬ ë°©ì‹: itemid ê¸°ë°˜ ê°œë³„ ì»¬ëŸ¼")
    print(f"   - ê²€ì‚¬ í•­ëª©: {metadata['data_summary']['total_lab_items']}ê°œ")
    print(f"   - ì…ì› ê±´ìˆ˜: {metadata['data_summary']['total_admissions']:,}ê±´")
    print(f"   - ê²€ì‚¬ ê¸°ë¡: {metadata['coverage']['total_lab_records']:,}ê±´")
    print(f"   - ì»¤ë²„ë¦¬ì§€: {metadata['coverage']['coverage_rate']:.1f}%")
    
    print(f"\nğŸ’¡ ì£¼ìš” íŠ¹ì§•:")
    print(f"   - ëª¨ë“  87ê°œ inclusion=1 itemid í¬í•¨")
    print(f"   - ì¤‘ë³µ ë¼ë²¨ ë°©ì§€ (itemidë³„ ê³ ìœ  ì»¬ëŸ¼)")
    print(f"   - Offset ì •ë³´ ë³„ë„ íŒŒì¼ ë¶„ë¦¬")
    print(f"   - ë°ì´í„° ì—†ëŠ” í•­ëª©ë„ ì»¬ëŸ¼ ìœ ì§€ (NaN)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "ğŸ¥ " * 20)
    print(" MIMIC-IV ì´ˆê¸° í˜ˆì•¡ê²€ì‚¬ ë°ì´í„° ì¶”ì¶œ (Clean Version)")
    print("ğŸ¥ " * 20)
    
    # 1. Inclusion=1 ê²€ì‚¬ í•­ëª© ë¡œë“œ
    LAB_ITEMS, LAB_METADATA = load_inclusion_items()
    
    # 2. ë°ì´í„° ë¡œë“œ
    admissions, patients, labevents = load_data()
    
    # 3. ì‹œê°„ ìœˆë„ìš° ê²€ì‚¬ ì¶”ì¶œ
    long_df, offset_df = extract_labs_with_window(admissions, labevents, LAB_ITEMS)
    
    # 4. Wide format ë³€í™˜
    wide_df, lab_columns = create_wide_format(admissions, long_df, LAB_ITEMS)
    
    # 5. ê²°ê³¼ ì €ì¥
    metadata = save_results(long_df, wide_df, offset_df, LAB_ITEMS, LAB_METADATA, lab_columns)
    
    # 6. ìµœì¢… ìš”ì•½
    print_final_summary(metadata)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("=" * 70)

if __name__ == "__main__":
    main()