#!/usr/bin/env python3
"""
MIMIC-IV hosp í•„ìˆ˜ í…Œì´ë¸” ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸
ìš”ì²­ëœ 5ê°œ í™˜ì í…Œì´ë¸”ê³¼ 4ê°œ ì‚¬ì „ í…Œì´ë¸” ì¶”ì¶œ
"""

import pandas as pd
import numpy as np
import os
import shutil
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì •
BASE_PATH = '/Users/hyungjun/Desktop/fast campus_lecture'
CHUNK_SIZE = 100000  # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ í¬ê¸°

def load_sampled_ids():
    """ìƒ˜í”Œëœ ID ë¡œë“œ"""
    print("1. ìƒ˜í”Œ ID ë¡œë”© ì¤‘...")
    
    # ìƒ˜í”Œë§ëœ admissionsì—ì„œ ID ì¶”ì¶œ
    admissions_path = os.path.join(BASE_PATH, 'processed_data/core/admissions_sampled.csv')
    admissions = pd.read_csv(admissions_path)
    
    subject_ids = set(admissions['subject_id'].unique())
    hadm_ids = set(admissions['hadm_id'].unique())
    
    print(f"âœ… ìƒ˜í”Œ ID ë¡œë“œ ì™„ë£Œ")
    print(f"   - Subject IDs: {len(subject_ids):,}ê°œ")
    print(f"   - Admission IDs: {len(hadm_ids):,}ê°œ")
    
    return subject_ids, hadm_ids

def copy_dictionary_tables():
    """ì‚¬ì „ í…Œì´ë¸” ë³µì‚¬ (ì „ì²´ ë°ì´í„°ì…‹ì— í•„ìš”)"""
    print("\n2. ì‚¬ì „ í…Œì´ë¸” ë³µì‚¬ ì¤‘...")
    
    dictionary_tables = [
        'd_hcpcs.csv',
        'd_icd_diagnoses.csv', 
        'd_icd_procedures.csv',
        'd_labitems.csv'
    ]
    
    source_dir = os.path.join(BASE_PATH, 'dataset2/hosp')
    target_dir = os.path.join(BASE_PATH, 'processed_data/hosp')
    
    copied_count = 0
    for table in dictionary_tables:
        source_path = os.path.join(source_dir, table)
        target_path = os.path.join(target_dir, table)
        
        if os.path.exists(source_path):
            shutil.copy2(source_path, target_path)
            file_size = os.path.getsize(target_path) / (1024 * 1024)  # MB
            print(f"   âœ… {table} ë³µì‚¬ ì™„ë£Œ ({file_size:.1f} MB)")
            copied_count += 1
        else:
            print(f"   âš ï¸ {table} íŒŒì¼ ì—†ìŒ")
    
    return copied_count

def extract_patient_table(table_name, subject_ids, hadm_ids, use_chunks=False):
    """í™˜ìë³„ ë°ì´í„° í…Œì´ë¸” ì¶”ì¶œ"""
    source_path = os.path.join(BASE_PATH, f'dataset2/hosp/{table_name}.csv')
    target_path = os.path.join(BASE_PATH, f'processed_data/hosp/{table_name}_sampled.csv')
    
    if not os.path.exists(source_path):
        print(f"   âš ï¸ {table_name} íŒŒì¼ ì—†ìŒ")
        return 0
    
    print(f"   ì²˜ë¦¬ ì¤‘: {table_name}...")
    
    try:
        if use_chunks:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì²­í¬ë¡œ ì²˜ë¦¬
            filtered_chunks = []
            total_rows = 0
            chunk_count = 0
            
            for chunk_num, chunk in enumerate(pd.read_csv(source_path, chunksize=CHUNK_SIZE)):
                # subject_id ë˜ëŠ” hadm_idë¡œ í•„í„°ë§
                if 'subject_id' in chunk.columns and 'hadm_id' in chunk.columns:
                    filtered = chunk[
                        (chunk['subject_id'].isin(subject_ids)) | 
                        (chunk['hadm_id'].isin(hadm_ids))
                    ]
                elif 'subject_id' in chunk.columns:
                    filtered = chunk[chunk['subject_id'].isin(subject_ids)]
                elif 'hadm_id' in chunk.columns:
                    filtered = chunk[chunk['hadm_id'].isin(hadm_ids)]
                else:
                    print(f"      âš ï¸ {table_name}: subject_id/hadm_id ì»¬ëŸ¼ ì—†ìŒ")
                    return 0
                
                if not filtered.empty:
                    filtered_chunks.append(filtered)
                    total_rows += len(filtered)
                
                chunk_count += 1
                if chunk_count % 100 == 0:
                    print(f"      ì²˜ë¦¬ ì¤‘... {chunk_count * CHUNK_SIZE:,} í–‰ ê²€ì‚¬, {total_rows:,} í–‰ ë§¤ì¹­")
            
            if filtered_chunks:
                result_df = pd.concat(filtered_chunks, ignore_index=True)
                result_df.to_csv(target_path, index=False)
                file_size = os.path.getsize(target_path) / (1024 * 1024)  # MB
                print(f"   âœ… {table_name}: {total_rows:,} í–‰ ì¶”ì¶œ ({file_size:.1f} MB)")
                return total_rows
            else:
                print(f"   âš ï¸ {table_name}: ë§¤ì¹­ ë°ì´í„° ì—†ìŒ")
                return 0
                
        else:
            # ì‘ì€ íŒŒì¼ì€ í•œ ë²ˆì— ì²˜ë¦¬
            df = pd.read_csv(source_path)
            original_rows = len(df)
            
            if 'subject_id' in df.columns and 'hadm_id' in df.columns:
                filtered = df[
                    (df['subject_id'].isin(subject_ids)) | 
                    (df['hadm_id'].isin(hadm_ids))
                ]
            elif 'subject_id' in df.columns:
                filtered = df[df['subject_id'].isin(subject_ids)]
            elif 'hadm_id' in df.columns:
                filtered = df[df['hadm_id'].isin(hadm_ids)]
            else:
                print(f"      âš ï¸ {table_name}: subject_id/hadm_id ì»¬ëŸ¼ ì—†ìŒ")
                return 0
            
            if not filtered.empty:
                filtered.to_csv(target_path, index=False)
                file_size = os.path.getsize(target_path) / (1024 * 1024)  # MB
                reduction_pct = (1 - len(filtered) / original_rows) * 100
                print(f"   âœ… {table_name}: {len(filtered):,} í–‰ ì¶”ì¶œ ({file_size:.1f} MB, {reduction_pct:.1f}% ê°ì†Œ)")
                return len(filtered)
            else:
                print(f"   âš ï¸ {table_name}: ë§¤ì¹­ ë°ì´í„° ì—†ìŒ")
                return 0
                
    except Exception as e:
        print(f"   âŒ {table_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ“Š MIMIC-IV Hosp í•„ìˆ˜ í…Œì´ë¸” ì¶”ì¶œ")
    print("=" * 80)
    
    start_time = datetime.now()
    
    # ê²°ê³¼ ì €ì¥ìš©
    extraction_stats = {
        'extraction_date': start_time.strftime('%Y-%m-%d %H:%M:%S'),
        'dictionary_tables': {},
        'patient_tables': {},
        'processing_time': None
    }
    
    # 1. ìƒ˜í”Œ ID ë¡œë“œ
    subject_ids, hadm_ids = load_sampled_ids()
    extraction_stats['sampled_subjects'] = len(subject_ids)
    extraction_stats['sampled_admissions'] = len(hadm_ids)
    
    # 2. ì‚¬ì „ í…Œì´ë¸” ë³µì‚¬
    dict_count = copy_dictionary_tables()
    extraction_stats['dictionary_tables']['count'] = dict_count
    extraction_stats['dictionary_tables']['files'] = [
        'd_hcpcs.csv', 'd_icd_diagnoses.csv', 
        'd_icd_procedures.csv', 'd_labitems.csv'
    ]
    
    # 3. í•„ìˆ˜ í™˜ì ë°ì´í„° í…Œì´ë¸” ì¶”ì¶œ
    print("\n3. í•„ìˆ˜ í™˜ì ë°ì´í„° í…Œì´ë¸” ì¶”ì¶œ ì¤‘...")
    
    # ìš”ì²­ëœ 5ê°œ í•„ìˆ˜ í…Œì´ë¸”
    essential_tables = [
        ('diagnoses_icd', False),      # ì§„ë‹¨ ì½”ë“œ
        ('drgcodes', False),           # DRG ì½”ë“œ
        # ('labevents', True),           # ê²€ì‚¬ ê²°ê³¼ (ëŒ€ìš©ëŸ‰) - ë³„ë„ ì²˜ë¦¬ í•„ìš”
        ('microbiologyevents', False), # ë¯¸ìƒë¬¼ ê²€ì‚¬
        ('services', False)            # ì§„ë£Œ ì„œë¹„ìŠ¤
    ]
    
    print("\n   âš ï¸ ì°¸ê³ : labeventsëŠ” ë§¤ìš° í° íŒŒì¼(1.2ì–µ í–‰)ì´ë¯€ë¡œ ë³„ë„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    for table_name, use_chunks in essential_tables:
        row_count = extract_patient_table(table_name, subject_ids, hadm_ids, use_chunks)
        extraction_stats['patient_tables'][table_name] = row_count
    
    # 4. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    extraction_stats['processing_time'] = f"{processing_time:.1f} seconds"
    
    # 5. í†µê³„ ì €ì¥
    print("\n4. ì¶”ì¶œ í†µê³„ ì €ì¥ ì¤‘...")
    stats_path = os.path.join(BASE_PATH, 'processed_data/hosp/extraction_stats.json')
    with open(stats_path, 'w') as f:
        json.dump(extraction_stats, f, indent=2)
    print(f"âœ… í†µê³„ ì €ì¥: extraction_stats.json")
    
    # 6. ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("âœ… Hosp í•„ìˆ˜ í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ!")
    print("=" * 80)
    
    print("\n[ì¶”ì¶œ ìš”ì•½]")
    print(f"â€¢ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
    print(f"â€¢ ìƒ˜í”Œ í™˜ì: {len(subject_ids):,}ëª…")
    print(f"â€¢ ìƒ˜í”Œ ì…ì›: {len(hadm_ids):,}ê±´")
    print(f"â€¢ ì‚¬ì „ í…Œì´ë¸”: {dict_count}ê°œ (ì „ì²´ ë³µì‚¬)")
    
    total_rows = sum(extraction_stats['patient_tables'].values())
    successful_tables = sum(1 for v in extraction_stats['patient_tables'].values() if v > 0)
    print(f"â€¢ í™˜ì ë°ì´í„°: {successful_tables}/5ê°œ í…Œì´ë¸”, ì´ {total_rows:,} í–‰")
    
    print("\n[í…Œì´ë¸”ë³„ í†µê³„]")
    for table, count in extraction_stats['patient_tables'].items():
        if count > 0:
            print(f"  âœ“ {table}: {count:,} í–‰")
        else:
            print(f"  âœ— {table}: ë°ì´í„° ì—†ìŒ")
    
    print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: processed_data/hosp/")
    print(f"ğŸ“Š í†µê³„ íŒŒì¼: processed_data/hosp/extraction_stats.json")
    
    return extraction_stats

if __name__ == "__main__":
    stats = main()