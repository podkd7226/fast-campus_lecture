# ğŸ¤– ì˜ˆì¸¡ ëª¨ë¸ë§ ê°€ì´ë“œ

## ğŸ“Œ ê°œìš”

ì´ ë¬¸ì„œëŠ” MIMIC-IV ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì‚¬ë§ë¥  ë° ì…ì›ê¸°ê°„ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ì‹¤ìš©ì  ê°€ì´ë“œì…ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3ê°€ì§€ ë ˆë²¨ì˜ ë°ì´í„°ì…‹ì„ ì œê³µí•˜ë©°, ê°ê°ì˜ í™œìš© ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

---

## ğŸ“Š ì œê³µ ë°ì´í„°ì…‹

> ğŸ“– **ê° ë°ì´í„°ì…‹ì˜ ìƒì„¸ ë¶„ì„**
> - [Essential Dataset ìƒì„¸ ë¶„ì„](../datasets/Essential_Dataset_Analysis.md)
> - [Extended Dataset ìƒì„¸ ë¶„ì„](../datasets/Extended_Dataset_Analysis.md)
> - [Comprehensive Dataset ìƒì„¸ ë¶„ì„](../datasets/Comprehensive_Dataset_Analysis.md)

### 1. Essential Dataset (í•„ìˆ˜ ë³€ìˆ˜ ì„¸íŠ¸)
- **íŒŒì¼**: `model_dataset_essential.csv`
- **í¬ê¸°**: 1,200 Ã— 20 (Lab 9ê°œ + ê¸°íƒ€ 11ê°œ)
- **í‰ê·  ê²°ì¸¡ë¥ **: 6.0%
- **ì™„ì „ ì¼€ì´ìŠ¤**: 1,096ê°œ (91.3%)

#### í¬í•¨ ë³€ìˆ˜
```python
lab_features = [
    'Hematocrit',      # ì í˜ˆêµ¬ ìš©ì ë¥ 
    'Hemoglobin',      # í—¤ëª¨ê¸€ë¡œë¹ˆ
    'Creatinine',      # ì‹ ì¥ ê¸°ëŠ¥
    'RDW',             # ì í˜ˆêµ¬ ë¶„í¬
    'White_Blood_Cells', # ë°±í˜ˆêµ¬
    'Urea_Nitrogen',   # ìš”ì†Œì§ˆì†Œ
    'Potassium',       # ì¹¼ë¥¨
    'Sodium',          # ë‚˜íŠ¸ë¥¨
    'Glucose'          # í˜ˆë‹¹
]
```

#### ê¶Œì¥ ì‚¬ìš© ì¼€ì´ìŠ¤
- âœ… **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•**
- âœ… **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**
- âœ… **ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ** (ëª¨ë“  ì…ì› í™˜ì ì ìš© ê°€ëŠ¥)
- âœ… **ê²°ì¸¡ê°’ ì²˜ë¦¬ ìµœì†Œí™”ê°€ í•„ìš”í•œ ê²½ìš°**

### 2. Extended Dataset (í™•ì¥ ë³€ìˆ˜ ì„¸íŠ¸)
- **íŒŒì¼**: `model_dataset_extended.csv`
- **í¬ê¸°**: 1,200 Ã— 28 (Lab 17ê°œ + ê¸°íƒ€ 11ê°œ)
- **í‰ê·  ê²°ì¸¡ë¥ **: 22.2%
- **ì™„ì „ ì¼€ì´ìŠ¤**: 0ê°œ (ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìˆ˜)

#### ì¶”ê°€ ë³€ìˆ˜
```python
additional_features = [
    'Basophils', 'Eosinophils',  # CBC ìƒì„¸
    'PT', 'PTT',                  # ì‘ê³  ê²€ì‚¬
    'Calcium_Total',               # ì¹¼ìŠ˜
    'Bilirubin_Total',            # ë¹Œë¦¬ë£¨ë¹ˆ
    'Lactate',                     # ì –ì‚° (ì¤‘ì¦ë„)
    'Platelet_Count'              # í˜ˆì†ŒíŒ
]
```

#### ê¶Œì¥ ì‚¬ìš© ì¼€ì´ìŠ¤
- âœ… **ê· í˜•ì¡íŒ ì˜ˆì¸¡ ëª¨ë¸**
- âœ… **ì¤‘ì¦ë„ í‰ê°€ê°€ ì¤‘ìš”í•œ ê²½ìš°** (Lactate í¬í•¨)
- âœ… **ì‘ê³  ì¥ì•  ìœ„í—˜ í‰ê°€ í•„ìš”ì‹œ**
- âš ï¸ ê²°ì¸¡ê°’ ëŒ€ì²´ ì „ëµ í•„ìˆ˜

### 3. Comprehensive Dataset (í¬ê´„ì  ë³€ìˆ˜ ì„¸íŠ¸)
- **íŒŒì¼**: `model_dataset_comprehensive.csv`
- **í¬ê¸°**: 1,200 Ã— 35 (Lab 24ê°œ + ê¸°íƒ€ 11ê°œ)
- **í‰ê·  ê²°ì¸¡ë¥ **: 37.5%
- **ì™„ì „ ì¼€ì´ìŠ¤**: 0ê°œ (ê³ ê¸‰ ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìˆ˜)

#### ì¶”ê°€ íŠ¹ìˆ˜ ê²€ì‚¬
```python
special_features = [
    'Albumin',           # ì˜ì–‘ ìƒíƒœ
    'pH', 'pO2', 'pCO2', # ë™ë§¥í˜ˆ ê°€ìŠ¤
    'Creatine_Kinase',   # ê·¼ìœ¡ ì†ìƒ
    'Troponin_T',        # ì‹¬ê·¼ ì†ìƒ
    'Chloride'           # ì „í•´ì§ˆ
]
```

#### ê¶Œì¥ ì‚¬ìš© ì¼€ì´ìŠ¤
- âœ… **ì—°êµ¬ìš© ê³ ê¸‰ ëª¨ë¸**
- âœ… **ì•™ìƒë¸” ëª¨ë¸ì˜ êµ¬ì„± ìš”ì†Œ**
- âœ… **íŠ¹ì • ì§ˆí™˜êµ° ì˜ˆì¸¡** (ì‹¬ì¥, í˜¸í¡ê¸° ë“±)
- âš ï¸ ë³µì¡í•œ ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìš”

---

## ğŸ¯ ì˜ˆì¸¡ íƒ€ê²Ÿ

### 1. ì‚¬ë§ ì˜ˆì¸¡

#### ì´ì§„ ë¶„ë¥˜ (Binary Classification)
```python
# ì „ì²´ ì‚¬ë§ ì˜ˆì¸¡
target = 'death_binary'  # 0: ìƒì¡´, 1: ì‚¬ë§

# ë³‘ì› ë‚´ ì‚¬ë§ ì˜ˆì¸¡
target = 'hospital_death'  # 0: ìƒì¡´/ë³‘ì› ì™¸ ì‚¬ë§, 1: ë³‘ì› ë‚´ ì‚¬ë§
```

#### ë‹¤ì¤‘ ë¶„ë¥˜ (Multi-class Classification)
```python
target = 'death_type'  # 'alive', 'hospital', 'outside'
```

### 2. ì…ì›ê¸°ê°„ ì˜ˆì¸¡

#### íšŒê·€ (Regression)
```python
target = 'los_days'   # ì—°ì†í˜• (ì¼ ë‹¨ìœ„)
target = 'los_hours'  # ì—°ì†í˜• (ì‹œê°„ ë‹¨ìœ„)
```

#### êµ¬ê°„ ë¶„ë¥˜ (Ordinal Classification)
```python
# ì…ì›ê¸°ê°„ì„ êµ¬ê°„ìœ¼ë¡œ ë³€í™˜
df['los_category'] = pd.cut(df['los_days'], 
                            bins=[0, 3, 7, 14, float('inf')],
                            labels=['short', 'medium', 'long', 'very_long'])
```

---

## ğŸ’¡ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ëµ

### 1. Essential Dataset (ê²°ì¸¡ë¥  6%)

```python
from sklearn.impute import SimpleImputer

# ë‹¨ìˆœ ëŒ€ì²´ (ì¤‘ì•™ê°’)
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# ë˜ëŠ” ì™„ì „ ì¼€ì´ìŠ¤ë§Œ ì‚¬ìš© (91.3% ë°ì´í„° ìœ ì§€)
df_complete = df.dropna()
```

### 2. Extended Dataset (ê²°ì¸¡ë¥  22%)

```python
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# KNN ëŒ€ì²´
knn_imputer = KNNImputer(n_neighbors=5)
X_imputed = knn_imputer.fit_transform(X)

# MICE (Iterative Imputation)
mice_imputer = IterativeImputer(random_state=42)
X_imputed = mice_imputer.fit_transform(X)
```

### 3. Comprehensive Dataset (ê²°ì¸¡ë¥  37%)

```python
# ê²°ì¸¡ ì§€ì‹œì ì¶”ê°€ (Missing Indicator)
from sklearn.impute import MissingIndicator

# ê²°ì¸¡ ì—¬ë¶€ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ì¶”ê°€
indicator = MissingIndicator()
mask = indicator.fit_transform(X)
X_with_indicator = np.hstack([X_imputed, mask])

# ë˜ëŠ” ì œê³µëœ indicator íŒŒì¼ ì‚¬ìš©
df_indicators = pd.read_csv('model_dataset_extended_with_indicators.csv')
```

### 4. ê³ ê¸‰ ì „ëµ: ê²°ì¸¡ íŒ¨í„´ í™œìš©

```python
# ê²°ì¸¡ íŒ¨í„´ ìì²´ë¥¼ íŠ¹ì§•ìœ¼ë¡œ
df['n_missing'] = df[lab_features].isna().sum(axis=1)
df['missing_rate'] = df[lab_features].isna().mean(axis=1)

# íŠ¹ì • ê²€ì‚¬ ê²°ì¸¡ì´ ì˜ë¯¸í•˜ëŠ” ì •ë³´
df['lactate_tested'] = (~df['Lactate'].isna()).astype(int)  # ì¤‘ì¦ë„ ì§€í‘œ
df['troponin_tested'] = (~df['Troponin_T'].isna()).astype(int)  # ì‹¬ì¥ ë¬¸ì œ ì˜ì‹¬
```

---

## ğŸ”§ ëª¨ë¸ ì¶”ì²œ

### 1. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Logistic Regression (í•´ì„ ê°€ëŠ¥)
lr_model = LogisticRegression(random_state=42)

# Random Forest (ê²°ì¸¡ê°’ì— ê°•ê±´)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
```

### 2. ê³ ê¸‰ ëª¨ë¸

```python
import xgboost as xgb
import lightgbm as lgb

# XGBoost (ê²°ì¸¡ê°’ ìë™ ì²˜ë¦¬)
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

# LightGBM (ëŒ€ìš©ëŸ‰ ë°ì´í„°, ë²”ì£¼í˜• ë³€ìˆ˜)
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    num_leaves=31,
    random_state=42
)
```

### 3. ë”¥ëŸ¬ë‹ ëª¨ë¸

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

model = Sequential([
    Dense(64, activation='relu', input_dim=n_features),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜
])
```

---

## ğŸ“ˆ í‰ê°€ ì§€í‘œ

### ì‚¬ë§ ì˜ˆì¸¡ (ë¶ˆê· í˜• ë°ì´í„°)

```python
from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score

# AUROC (ì „ì²´ì  ì„±ëŠ¥)
auroc = roc_auc_score(y_true, y_pred_proba)

# AUPRC (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)
precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
auprc = auc(recall, precision)

# F1 Score (ê· í˜•ì¡íŒ ì§€í‘œ)
f1 = f1_score(y_true, y_pred)
```

### ì…ì›ê¸°ê°„ ì˜ˆì¸¡

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# MAE (ì¼ ë‹¨ìœ„ í‰ê·  ì˜¤ì°¨)
mae = mean_absolute_error(y_true, y_pred)

# RMSE (í° ì˜¤ì°¨ì— ë¯¼ê°)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))

# RÂ² (ì„¤ëª…ë ¥)
r2 = r2_score(y_true, y_pred)
```

---

## ğŸš€ ì‹¤ì „ ì˜ˆì œ

### ì™„ì „ íŒŒì´í”„ë¼ì¸ ì˜ˆì œ

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('model_dataset_essential.csv')

# 2. íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
feature_cols = [col for col in df.columns 
                if col not in ['hadm_id', 'subject_id', 'death_type', 
                              'death_binary', 'hospital_death', 'los_hours', 'los_days']]
X = df[feature_cols]
y = df['death_binary']

# 3. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. ì „ì²˜ë¦¬
# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
X_train_encoded = pd.get_dummies(X_train, columns=['gender', 'admission_type'])
X_test_encoded = pd.get_dummies(X_test, columns=['gender', 'admission_type'])

# ê²°ì¸¡ê°’ ì²˜ë¦¬
imputer = SimpleImputer(strategy='median')
X_train_imputed = imputer.fit_transform(X_train_encoded.select_dtypes(include=[np.number]))
X_test_imputed = imputer.transform(X_test_encoded.select_dtypes(include=[np.number]))

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# 5. ëª¨ë¸ í›ˆë ¨
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 6. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

print(classification_report(y_test, y_pred))
print(f"AUROC: {roc_auc_score(y_test, y_pred_proba):.3f}")
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ë°ì´í„° ëˆ„ìˆ˜ (Data Leakage)
- `hospital_expire_flag`ëŠ” `hospital_death`ì™€ ë™ì¼í•˜ë¯€ë¡œ ì œê±°
- `deathtime`ì€ íƒ€ê²Ÿ ì •ë³´ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ì œê±°

### 2. ì‹œê°„ì  ê²€ì¦
- ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ì‹œê°„ ê¸°ë°˜ ë¶„í•  ê³ ë ¤
- Cross-validation ì‹œ í™˜ì ë‹¨ìœ„ ë¶„í•  (ê°™ì€ í™˜ìì˜ ì—¬ëŸ¬ ì…ì› ë¶„ë¦¬)

### 3. í•´ì„ ê°€ëŠ¥ì„±
- ì˜ë£Œ ë¶„ì•¼ì—ì„œëŠ” ëª¨ë¸ í•´ì„ì´ ì¤‘ìš”
- SHAP, LIME ë“± ì„¤ëª… ê°€ëŠ¥í•œ AI ê¸°ë²• í™œìš©

```python
import shap

# SHAP ê°’ ê³„ì‚°
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# ì‹œê°í™”
shap.summary_plot(shap_values, X_test)
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- [MIMIC-IV ë‚˜ì´ ê³„ì‚° ê°€ì´ë“œ](./MIMIC_IV_Age_Calculation_Guide.md)
- [ê²°ì¸¡ê°’ ë¶„ì„ ë³´ê³ ì„œ](../../../analysis_initial_lab/missing_value_analysis.md)
- [XGBoost ë¬¸ì„œ](https://xgboost.readthedocs.io/)
- [Scikit-learn Imputation](https://scikit-learn.org/stable/modules/impute.html)

---

*ì‘ì„±ì¼: 2025-01-06*  
*ë°ì´í„°: MIMIC-IV ìƒ˜í”Œ 1,200ê±´*